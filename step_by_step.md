
# üöÄ Guia de Implementa√ß√£o: Smart Reply Assistant

Este documento cont√©m o passo a passo consolidado para configurar e executar o assistente de resposta inteligente utilizando **Python** e **Llama 3.2** local.

## üõ†Ô∏è Passo 1: Prepara√ß√£o do Ambiente

Considerando um ambiente Windows com VS Code, execute os comandos abaixo no terminal:

1. **Cria√ß√£o da pasta e ambiente virtual:**
```powershell
mkdir smart-reply-assistant
cd smart-reply-assistant
python -m venv venv

```


2. **Ativa√ß√£o do Ambiente (PowerShell):**
*Caso ocorra erro de permiss√£o, execute:* Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
*Ative o venv:*
```powershell
.\venv\Scripts\activate

```


3. **Instala√ß√£o de Recursos e Depend√™ncias:**
```powershell
# Instala√ß√£o do Ollama (caso n√£o tenha)
winget install ollama

# Baixar o modelo Llama 3.2
ollama pull llama3.2

# Instala√ß√£o das bibliotecas Python necess√°rias
pip install requests streamlit

```



---

## üß† Passo 2: O Motor de IA (`ai_engine.py`)

Crie o arquivo na raiz do projeto (**fora** da pasta venv). Ele √© respons√°vel pela comunica√ß√£o direta com o Ollama.

```python
import requests

def generate_ai_response(prompt, model="llama3.2"):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }

    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json().get("response", "Erro: Resposta vazia.")
    except Exception as e:
        return f"Erro ao conectar com Ollama: {str(e)}"

```

---

## üìù Passo 3: Engenharia de Prompt (`prompts.py`)

Neste arquivo, definimos as "regras de neg√≥cio" e o formato de sa√≠da obrigat√≥rio (JSON).

```python
SYSTEM_INSTRUCTION = """
Voc√™ √© um assistente de atendimento ao cliente profissional.
Sua tarefa √© analisar a mensagem do cliente e gerar EXATAMENTE tr√™s op√ß√µes de resposta no formato JSON.

As op√ß√µes devem ser:
1. "curta": Uma resposta r√°pida e direta.
2. "detalhada": Uma resposta emp√°tica e explicativa.
3. "comercial": Uma resposta focada em convers√£o ou fideliza√ß√£o.

FORMATO DE RESPOSTA (Siga estritamente):
{
  "curta": "texto",
  "detalhada": "texto",
  "comercial": "texto"
}
"""

def format_reply_prompt(customer_message):
    return f"{SYSTEM_INSTRUCTION}\n\nMensagem do Cliente: {customer_message}\n\nRetorne apenas o JSON:"

```

---

## ‚öôÔ∏è Passo 4: L√≥gica de Processamento (`backend.py`)

Respons√°vel por unir o prompt √† IA e tratar os dados recebidos.

```python
import json
from ai_engine import generate_ai_response
from prompts import format_reply_prompt

def process_customer_message(user_input):
    prompt_completo = format_reply_prompt(user_input)
    resposta_bruta = generate_ai_response(prompt_completo)
    
    try:
        # Converte a string da IA em dicion√°rio Python
        return json.loads(resposta_bruta)
    except:
        # Fallback de seguran√ßa caso o JSON venha mal formatado
        return {
            "curta": "Erro de formata√ß√£o.",
            "detalhada": resposta_bruta,
            "comercial": "Erro de formata√ß√£o."
        }

```

---

## üé® Passo 5: Interface Visual (`app.py`)

O "rosto" do projeto, constru√≠do com Streamlit para exibi√ß√£o em abas.

```python
import streamlit as st
from backend import process_customer_message

st.set_page_config(page_title="Smart Reply Assistant", page_icon="ü§ñ")

st.title("ü§ñ Smart Reply Assistant")
st.markdown("Interface para gera√ß√£o de respostas inteligentes via IA local.")

customer_input = st.text_area("Mensagem do cliente:", height=150)

if st.button("Gerar Sugest√µes"):
    if customer_input.strip():
        with st.spinner("O Llama 3.2 est√° gerando as op√ß√µes..."):
            opcoes = process_customer_message(customer_input)
            
            tab1, tab2, tab3 = st.tabs(["‚ö° Curta", "üìù Detalhada", "üí∞ Comercial"])
            
            with tab1:
                st.info(opcoes.get("curta"))
            with tab2:
                st.success(opcoes.get("detalhada"))
            with tab3:
                st.warning(opcoes.get("comercial"))
    else:
        st.error("Por favor, digite uma mensagem primeiro.")

```

---

## ‚ñ∂Ô∏è Como Rodar o Projeto

1. Certifique-se de que o **Ollama** est√° ativo.
2. No terminal do VS Code (com venv ativo), execute:
```powershell
streamlit run app.py

```