# ü§ñ Smart Reply Assistant (Local AI)

> Um assistente inteligente de suporte ao cliente que roda 100% offline, garantindo privacidade e agilidade.

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Streamlit](https://img.shields.io/badge/Frontend-Streamlit-red)
![Ollama](https://img.shields.io/badge/AI-Ollama%20(Llama%203.2)-orange)
![Status](https://img.shields.io/badge/Status-Funcional-brightgreen)

## üìñ Sobre o Projeto

O **Smart Reply Assistant** √© uma aplica√ß√£o Fullstack (Backend Python + Frontend Streamlit) desenvolvida para auxiliar equipes de atendimento ao cliente. 

Diferente de chatbots comuns, este sistema utiliza **Intelig√™ncia Artificial Generativa Local (Llama 3.2)** para analisar a mensagem do cliente e sugerir **tr√™s op√ß√µes de resposta** com tons diferentes (Curta, Detalhada e Comercial), permitindo que o humano tome a decis√£o final.

### üöÄ Destaques T√©cnicos
* **Privacidade Total:** Utiliza o Ollama rodando localmente. Nenhum dado de cliente sai do servidor/computador.
* **Structured Output (JSON):** A IA n√£o retorna apenas texto; ela √© for√ßada via *Prompt Engineering* a retornar um objeto JSON estruturado, garantindo integra√ß√£o perfeita com o c√≥digo Python.
* **Arquitetura Modular:** Separa√ß√£o clara de responsabilidades entre Motor de IA, Regras de Neg√≥cio (Prompts), Backend e Interface.

---

## üõ†Ô∏è Pr√©-requisitos (Instala√ß√£o)

Se voc√™ est√° come√ßando do zero e tem apenas o VS Code instalado no Windows, siga os passos abaixo para preparar seu computador.

### 1. Instalar Python e Ollama
Abra o seu **PowerShell** (Terminal do Windows) e execute:

```powershell
# 1. Instalar o Ollama (Motor da IA)
winget install ollama

# 2. Baixar o modelo Llama 3.2 (C√©rebro da IA)
ollama run llama3.2
# (Ap√≥s o download, digite /bye para sair do chat e deixar o servidor rodando)

‚öôÔ∏è Instala√ß√£o do Projeto
1. Clonar e preparar a pasta
mkdir smart-reply-assistant
cd smart-reply-assistant

2. Criar Ambiente Virtual (Venv)
Isso isola as bibliotecas do projeto para n√£o bagun√ßar seu Windows.
python -m venv venv
# Ativar o ambiente (Obrigat√≥rio toda vez que for mexer no projeto)
.\venv\Scripts\activate
(Se der erro de permiss√£o no PowerShell, rode: Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process e tente ativar de novo).

3. Instalar Depend√™ncias
Com o (venv) aparecendo no terminal, instale as bibliotecas necess√°rias:
pip install streamlit requests


üìÇ Estrutura do Projeto
Certifique-se de que seus arquivos estejam organizados desta forma na pasta raiz:
smart-reply-assistant/
‚îÇ
‚îú‚îÄ‚îÄ ai_engine.py      # Conex√£o HTTP com a API local do Ollama
‚îú‚îÄ‚îÄ prompts.py        # Engenharia de Prompt e Regras de Comportamento
‚îú‚îÄ‚îÄ backend.py        # L√≥gica principal e tratamento de JSON
‚îú‚îÄ‚îÄ app.py            # Interface visual (Frontend Streamlit)
‚îî‚îÄ‚îÄ venv/             # Pasta do ambiente virtual (N√ÉO MEXER)

‚ñ∂Ô∏è Como Executar
1 - Certifique-se de que o Ollama est√° rodando (o √≠cone deve aparecer na bandeja do sistema, perto do rel√≥gio).
2 - No terminal do VS Code (com o venv ativo), execute: streamlit run app.py
3 - O navegador abrir√° automaticamente no endere√ßo http://localhost:8501.

üß† Detalhes de Implementa√ß√£o
Engenharia de Prompt (prompts.py)
Utilizamos a t√©cnica de Few-Shot Prompting e Output Enforcement, o sistema instrui o modelo Llama 3.2 a ignorar sa√≠das de texto livre e focar estritamente em um formato JSON:
{
  "curta": "Resposta direta...",
  "detalhada": "Resposta emp√°tica...",
  "comercial": "Oferta ou cupom..."
}
Tratamento de Erros (backend.py)
O sistema possui resili√™ncia implementada, caso a IA "alucine" e n√£o entregue um JSON v√°lido, o Python captura a exce√ß√£o (try/except) e exibe uma mensagem de erro amig√°vel, impedindo que a aplica√ß√£o trave na tela do usu√°rio.


Essa √© uma das partes mais valiosas de um reposit√≥rio para um desenvolvedor **J√∫nior**, pois mostra que voc√™ n√£o apenas sabe construir, mas tamb√©m sabe **diagnosticar e documentar problemas comuns**.

Aqui est√° a se√ß√£o de **Resolu√ß√£o de Problemas** formatada para voc√™ adicionar ao final do seu `README.md`. Ela cobre exatamente o que passamos hoje:

---

## üÜò Resolu√ß√£o de Problemas (Troubleshooting)

Se voc√™ encontrar dificuldades ao rodar o projeto, verifique as solu√ß√µes abaixo:

Para o seu arquivo **README.md**, uma se√ß√£o de "Solu√ß√£o de Problemas" (ou *Troubleshooting*) √© essencial para ajudar outros desenvolvedores (ou voc√™ mesmo no futuro) a resolver gargalos comuns rapidamente.

Aqui est√° a estrutura pronta para copiar e colar:

---

## üõ†Ô∏è Solu√ß√£o de Problemas (Troubleshooting)

Se algo n√£o funcionar como esperado, verifique estes pontos comuns:

### üíª Erros de Terminal e Ambiente

* **Comando `source` n√£o funciona:** * **Por que ocorre?** Voc√™ est√° usando comandos de Linux no PowerShell do Windows.
* **Solu√ß√£o:** Utilize `.\venv\Scripts\activate`.


* **Erro de Permiss√£o (`PSSecurityException`):** * **Por que ocorre?** O Windows bloqueia a execu√ß√£o de scripts por seguran√ßa.
* **Solu√ß√£o:** No terminal, execute:
`Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process`.



---

### üì¶ Erros de Bibliotecas (Python/Streamlit)

* **Erro: `module 'streamlit' has no attribute 'copy_button'`:** * **Por que ocorre?** A vers√£o do Streamlit dentro do seu `venv` est√° desatualizada (precisa ser > 1.22).
* **Solu√ß√£o:** Com o ambiente ativo, force a atualiza√ß√£o:
`python -m pip install --upgrade streamlit`.



---

### üß† Erros de IA e Conex√£o

* **Falha ao conectar com o Ollama:** * **Por que ocorre?** O servidor do Ollama n√£o est√° rodando ou o modelo n√£o foi baixado.
* **Solu√ß√£o:** Verifique se o √≠cone do Ollama est√° na bandeja do sistema e se voc√™ executou `ollama pull llama3.2`.


* **Erro ao formatar JSON (Resposta Estranha):** * **Por que ocorre?** A IA pode ter enviado texto explicativo antes do c√≥digo.
* **Solu√ß√£o:** Clique em "Gerar Sugest√µes" novamente. O sistema possui um mecanismo de *fallback* para tratar esses casos.



---


