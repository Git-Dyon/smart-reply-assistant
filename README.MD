# ü§ñ Smart Reply Assistant (Local AI)

> Um assistente inteligente de suporte ao cliente que roda 100% offline, garantindo privacidade e agilidade.

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Streamlit](https://img.shields.io/badge/Frontend-Streamlit-red)
![Ollama](https://img.shields.io/badge/AI-Ollama%20(Llama%203.2)-orange)
![Status](https://img.shields.io/badge/Status-Funcional-brightgreen)

## üìñ Sobre o Projeto

O **Smart Reply Assistant** √© uma aplica√ß√£o Fullstack (Backend Python + Frontend Streamlit) desenvolvida para auxiliar equipes de atendimento ao cliente. 

Diferente de chatbots comuns, este sistema utiliza **Intelig√™ncia Artificial Generativa Local (Llama 3.2)** para analisar a mensagem do cliente e sugerir **tr√™s op√ß√µes de resposta** com tons diferentes (Curta, Detalhada e Comercial), permitindo que o humano tome a decis√£o final.

### üöÄ Destaques T√©cnicos
* **Privacidade Total:** Utiliza o Ollama rodando localmente. Nenhum dado de cliente sai do servidor/computador.
* **Structured Output (JSON):** A IA n√£o retorna apenas texto; ela √© for√ßada via *Prompt Engineering* a retornar um objeto JSON estruturado, garantindo integra√ß√£o perfeita com o c√≥digo Python.
* **Arquitetura Modular:** Separa√ß√£o clara de responsabilidades entre Motor de IA, Regras de Neg√≥cio (Prompts), Backend e Interface.

---

## üõ†Ô∏è Pr√©-requisitos (Instala√ß√£o)

Se voc√™ est√° come√ßando do zero e tem apenas o VS Code instalado no Windows, siga os passos abaixo para preparar seu computador.

### 1. Instalar Python e Ollama
Abra o seu **PowerShell** (Terminal do Windows) e execute:

```powershell
# 1. Instalar o Ollama (Motor da IA)
winget install ollama

# 2. Baixar o modelo Llama 3.2 (C√©rebro da IA)
ollama run llama3.2
# (Ap√≥s o download, digite /bye para sair do chat e deixar o servidor rodando)

‚öôÔ∏è Instala√ß√£o do Projeto
1. Clonar e preparar a pasta
mkdir smart-reply-assistant
cd smart-reply-assistant

2. Criar Ambiente Virtual (Venv)
Isso isola as bibliotecas do projeto para n√£o bagun√ßar seu Windows.
python -m venv venv
# Ativar o ambiente (Obrigat√≥rio toda vez que for mexer no projeto)
.\venv\Scripts\activate
(Se der erro de permiss√£o no PowerShell, rode: Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process e tente ativar de novo).

3. Instalar Depend√™ncias
Com o (venv) aparecendo no terminal, instale as bibliotecas necess√°rias:
pip install streamlit requests


üìÇ Estrutura do Projeto
Certifique-se de que seus arquivos estejam organizados desta forma na pasta raiz:
smart-reply-assistant/
‚îÇ
‚îú‚îÄ‚îÄ ai_engine.py      # Conex√£o HTTP com a API local do Ollama
‚îú‚îÄ‚îÄ prompts.py        # Engenharia de Prompt e Regras de Comportamento
‚îú‚îÄ‚îÄ backend.py        # L√≥gica principal e tratamento de JSON
‚îú‚îÄ‚îÄ app.py            # Interface visual (Frontend Streamlit)
‚îî‚îÄ‚îÄ venv/             # Pasta do ambiente virtual (N√ÉO MEXER)

‚ñ∂Ô∏è Como Executar
1 - Certifique-se de que o Ollama est√° rodando (o √≠cone deve aparecer na bandeja do sistema, perto do rel√≥gio).
2 - No terminal do VS Code (com o venv ativo), execute: streamlit run app.py
3 - O navegador abrir√° automaticamente no endere√ßo http://localhost:8501.

üß† Detalhes de Implementa√ß√£o
Engenharia de Prompt (prompts.py)
Utilizamos a t√©cnica de Few-Shot Prompting e Output Enforcement, o sistema instrui o modelo Llama 3.2 a ignorar sa√≠das de texto livre e focar estritamente em um formato JSON:
{
  "curta": "Resposta direta...",
  "detalhada": "Resposta emp√°tica...",
  "comercial": "Oferta ou cupom..."
}
Tratamento de Erros (backend.py)
O sistema possui resili√™ncia implementada, caso a IA "alucine" e n√£o entregue um JSON v√°lido, o Python captura a exce√ß√£o (try/except) e exibe uma mensagem de erro amig√°vel, impedindo que a aplica√ß√£o trave na tela do usu√°rio.


Essa √© uma das partes mais valiosas de um reposit√≥rio para um desenvolvedor **J√∫nior**, pois mostra que voc√™ n√£o apenas sabe construir, mas tamb√©m sabe **diagnosticar e documentar problemas comuns**.

Aqui est√° a se√ß√£o de **Resolu√ß√£o de Problemas** formatada para voc√™ adicionar ao final do seu `README.md`. Ela cobre exatamente o que passamos hoje:

---

## üÜò Resolu√ß√£o de Problemas (Troubleshooting)

Se voc√™ encontrar dificuldades ao rodar o projeto, verifique as solu√ß√µes abaixo:

| Problema | Causa Prov√°vel | Solu√ß√£o |
| --- | --- | --- |
| `O termo 'source' n√£o √© reconhecido` | Tentativa de usar comando Linux no Windows PowerShell. | Use `.\venv\Scripts\activate` para ativar o ambiente. |
| `Erro: PSSecurityException` | Pol√≠tica de execu√ß√£o do Windows bloqueando scripts. | Rode `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process` no terminal. |
| `AttributeError: streamlit has no attribute 'copy_button'` | Vers√£o do Streamlit no `venv` √© inferior a 1.22.0. | Com o venv ativo, rode `python -m pip install --upgrade streamlit`. |
| `Erro ao conectar com Ollama` | O servidor local do Ollama n√£o est√° ativo ou o modelo n√£o foi baixado. | Certifique-se de que o √≠cone do Ollama aparece na bandeja do sistema e que voc√™ rodou `ollama pull llama3.2`. |
| `Erro ao formatar JSON` | A IA retornou texto explicativo fora do bloco JSON esperado. | Refine o `prompts.py` ou tente gerar novamente a resposta (o sistema possui tratamento de erro via `try-except`). |
